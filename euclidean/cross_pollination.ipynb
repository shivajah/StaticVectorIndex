{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6602c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "import numpy as np\n",
    "import faiss\n",
    "import h5py\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "import bisect\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b37a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Utilities ---\n",
    "\n",
    "def download_fashion_mnist(cache_path, url):\n",
    "    if not os.path.exists(cache_path):\n",
    "        print(\"Downloading Fashion-MNIST (~300 MB)â€¦\")\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            with open(cache_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "        else:\n",
    "            raise Exception(f\"Failed to download dataset: HTTP{response.status_code}\")\n",
    "\n",
    "def load_fashion_mnist(cache_path):\n",
    "    with h5py.File(cache_path, \"r\") as f:\n",
    "        xb = f[\"train\"][:].astype(np.float32)\n",
    "        xq = f[\"test\"][:].astype(np.float32)\n",
    "        gt = f[\"neighbors\"][:]\n",
    "    return xb, xq, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e30d11d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Indexing Utilities ---\n",
    "\n",
    "def build_kmeans(xb, d, n_clusters, niter=20):\n",
    "    train_sample = xb[np.random.choice(len(xb), size=min(50000, len(xb)), replace=False)]\n",
    "    kmeans = faiss.Kmeans(d, n_clusters, niter=niter, verbose=True, spherical=False)\n",
    "    kmeans.train(train_sample)\n",
    "    return kmeans\n",
    "\n",
    "def assign_vectors_to_clusters(xb, kmeans, n_assign):\n",
    "    _, assignments = kmeans.index.search(xb, n_assign)\n",
    "    return assignments\n",
    "\n",
    "def cross_pollinate_metadata(xb, xb_inner_assignments, inner_centroids, inner_to_outer, N_CROSS):\n",
    "    vector_metadata = defaultdict(dict)\n",
    "    for idx, inner_ids in enumerate(xb_inner_assignments):\n",
    "        for inner_id in inner_ids[:N_CROSS]:\n",
    "            outer_id = inner_to_outer[inner_id][0]\n",
    "            centroid = inner_centroids[inner_id]\n",
    "            vec = xb[idx]\n",
    "            dist = np.linalg.norm(vec - centroid)\n",
    "            cos_sim = np.dot(vec, centroid) / (np.linalg.norm(vec) * np.linalg.norm(centroid) + 1e-8)\n",
    "            vector_metadata[outer_id].setdefault(inner_id, []).append((dist, cos_sim, idx))\n",
    "    # Sort each list by Euclidean distance to centroid\n",
    "    for outer_id in vector_metadata:\n",
    "        for inner_id in vector_metadata[outer_id]:\n",
    "            vector_metadata[outer_id][inner_id].sort()\n",
    "    return vector_metadata\n",
    "\n",
    "def build_outer_to_num_inner_clusters(vector_metadata):\n",
    "    \"\"\"\n",
    "    Build a dictionary mapping each outer cluster id to the number of inner clusters it contains.\n",
    "    Args:\n",
    "        vector_metadata: dict[outer_id][inner_id] -> list of vectors\n",
    "    Returns:\n",
    "        outer_to_num_inner_clusters: dict[outer_id] -> int (number of inner clusters)\n",
    "    \"\"\"\n",
    "    outer_to_num_inner_clusters = {}\n",
    "    for outer_id in vector_metadata:\n",
    "        outer_to_num_inner_clusters[outer_id] = len(vector_metadata[outer_id])\n",
    "\n",
    "    print(\"\\nNumber of inner clusters for each outer cluster:\")\n",
    "    for outer_id in sorted(outer_to_num_inner_clusters):\n",
    "        print(f\"Outer cluster {outer_id}: {outer_to_num_inner_clusters[outer_id]} inner clusters\")\n",
    "    \n",
    "    return outer_to_num_inner_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee892376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Search Utilities ---\n",
    "def pad_to_k(arr, k, pad_value):\n",
    "    arr = list(arr)\n",
    "    if len(arr) < k:\n",
    "        arr += [pad_value] * (k - len(arr))\n",
    "    return arr[:k]\n",
    "\n",
    "def search_query_cross_pollination(\n",
    "    x, outer_ids, inner_kmeans, vector_metadata, xb, k, d,\n",
    "    N_PROBE=1, probe_strategy=\"nprobe\", tshirt_size=\"small\", n_outer_total=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Probes clusters using either N_PROBE or T-SHIRT size strategy.\n",
    "    T-SHIRT size: Small/Medium/Large, each probes a percentage of outer and inner clusters.\n",
    "    \"\"\"\n",
    "    # T-SHIRT size settings\n",
    "    tshirt_settings = {\n",
    "        \"small\": 0.10,\n",
    "        \"medium\": 0.20,\n",
    "        \"large\": 0.30\n",
    "    }\n",
    "    best_heap = []\n",
    "    tau = float(\"inf\")\n",
    "    probed_inner_ids = set()\n",
    "    inner_probed = 0\n",
    "\n",
    "    if probe_strategy == \"tshirt\":\n",
    "        pct = tshirt_settings[tshirt_size]\n",
    "        n_outer_probe = max(1, int(np.ceil(n_outer_total * pct)))\n",
    "        outer_ids = outer_ids[:n_outer_probe]\n",
    "    else:\n",
    "        # nprobe strategy: probe as many outer clusters as needed to reach N_PROBE inner clusters\n",
    "        n_outer_probe = len(outer_ids)\n",
    "\n",
    "    outer_idx = 0\n",
    "    total_outer = len(outer_ids)\n",
    "\n",
    "    while (probe_strategy == \"nprobe\" and inner_probed < N_PROBE and outer_idx < total_outer) or \\\n",
    "          (probe_strategy == \"tshirt\" and outer_idx < total_outer):\n",
    "        outer_id = outer_ids[outer_idx]\n",
    "        outer_idx += 1\n",
    "        if outer_id not in vector_metadata:\n",
    "            continue\n",
    "        inner_ids = list(vector_metadata[outer_id].keys())\n",
    "        if not inner_ids:\n",
    "            continue\n",
    "        if probe_strategy == \"tshirt\":\n",
    "            n_inner_probe = max(1, int(np.ceil(len(inner_ids) * tshirt_settings[tshirt_size])))\n",
    "        else:\n",
    "            n_inner_probe = min(N_PROBE - inner_probed, len(inner_ids))\n",
    "        inner_ids_to_probe = [iid for iid in inner_ids if iid not in probed_inner_ids][:n_inner_probe]\n",
    "        if not inner_ids_to_probe:\n",
    "            continue\n",
    "        inner_centroids_subset = inner_kmeans.centroids[inner_ids_to_probe]\n",
    "        index_l2 = faiss.IndexFlatL2(d)\n",
    "        index_l2.add(inner_centroids_subset)\n",
    "        _, inner_ranks_local = index_l2.search(x.reshape(1, -1), len(inner_ids_to_probe))\n",
    "        selected_inner_ids = [inner_ids_to_probe[j] for j in inner_ranks_local[0] if j < len(inner_ids_to_probe)]\n",
    "        for inner_id in selected_inner_ids:\n",
    "            probed_inner_ids.add(inner_id)\n",
    "            idxs_meta = vector_metadata[outer_id][inner_id]\n",
    "            if not idxs_meta:\n",
    "                continue\n",
    "            centroid = inner_kmeans.centroids[inner_id]\n",
    "            d_qc = np.linalg.norm(x - centroid)\n",
    "            for dist_ic, cos_theta, idx2 in idxs_meta:\n",
    "                lower_bound = abs(d_qc - dist_ic)\n",
    "                if lower_bound > tau:\n",
    "                    continue\n",
    "                est_dist = np.sqrt(max(0.0, d_qc ** 2 + dist_ic ** 2 - 2 * d_qc * dist_ic * cos_theta))\n",
    "                if est_dist > tau:\n",
    "                    continue\n",
    "                actual_dist = np.linalg.norm(x - xb[idx2])\n",
    "                best_heap.append((actual_dist, idx2))\n",
    "                if len(best_heap) > k:\n",
    "                    best_heap.sort()\n",
    "                    best_heap = best_heap[:k]\n",
    "                    tau = best_heap[-1][0]\n",
    "            inner_probed += 1\n",
    "            if probe_strategy == \"nprobe\" and inner_probed >= N_PROBE:\n",
    "                break\n",
    "    return best_heap\n",
    "\n",
    "def search_all_queries_cross_pollination(\n",
    "    xq, xq_outer_assignments, inner_kmeans, vector_metadata, xb, k, d, gt,\n",
    "    N_PROBE=1, probe_strategy=\"nprobe\", tshirt_size=\"small\", n_outer_total=10\n",
    "):\n",
    "    I = []\n",
    "    D = []\n",
    "    start_time = time.time()\n",
    "    for i, x in enumerate(xq):\n",
    "        outer_ids = xq_outer_assignments[i]\n",
    "        best_heap = search_query_cross_pollination(\n",
    "            x, outer_ids, inner_kmeans, vector_metadata, xb, k, d,\n",
    "            N_PROBE=N_PROBE, probe_strategy=probe_strategy, tshirt_size=tshirt_size, n_outer_total=n_outer_total\n",
    "        )\n",
    "        if best_heap:\n",
    "            best_heap.sort()\n",
    "            idxs = [idx for _, idx in best_heap]\n",
    "            dists = [dist for dist, _ in best_heap]\n",
    "            I.append(pad_to_k(idxs, k, -1))  # Use -1 or another invalid index as pad\n",
    "            D.append(pad_to_k(dists, k, float('inf')))\n",
    "        else:\n",
    "            dists = np.linalg.norm(xb - x.reshape(1, -1), axis=1)\n",
    "            idx = np.argsort(dists)[:k]\n",
    "            I.append(idx)\n",
    "            D.append(dists[idx])\n",
    "    D = np.array(D)\n",
    "    I = np.array(I)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    qps = len(xq) / elapsed_time\n",
    "    recall = (I == gt[:, :k]).sum() / (gt.shape[0] * k)\n",
    "    return I, D, recall, qps                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b4cdb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experiment Utilities ---\n",
    "\n",
    "def evaluate_cross_pollination(\n",
    "    xb, xq, gt, inner_kmeans, inner_to_outer, xq_outer_assignments, k, d,\n",
    "    N_PROBE=1, min_cross=1, max_cross=5, probe_strategy=\"nprobe\", tshirt_size=\"small\", n_outer_total=10\n",
    "):\n",
    "    recalls = []\n",
    "    qps_list = []\n",
    "    cross_range = range(min_cross, max_cross + 1)\n",
    "    inner_centroids = inner_kmeans.centroids\n",
    "    for N_CROSS in cross_range:\n",
    "        print(f\"Evaluating N_CROSS = {N_CROSS}\")\n",
    "        xb_inner_assignments = assign_vectors_to_clusters(xb, inner_kmeans, N_CROSS)\n",
    "        vector_metadata = cross_pollinate_metadata(\n",
    "            xb, xb_inner_assignments, inner_centroids, inner_to_outer, N_CROSS\n",
    "        )\n",
    "        outer_to_num_inner_clusters = build_outer_to_num_inner_clusters(vector_metadata)\n",
    "        I, D, recall, qps = search_all_queries_cross_pollination(\n",
    "            xq, xq_outer_assignments, inner_kmeans, vector_metadata, xb, k, d, gt,\n",
    "            N_PROBE=N_PROBE, probe_strategy=probe_strategy, tshirt_size=tshirt_size, n_outer_total=n_outer_total\n",
    "        )\n",
    "        recalls.append(recall)\n",
    "        qps_list.append(qps)\n",
    "        print(f\"N_CROSS={N_CROSS}: recall={recall:.4f}, qps={qps:.2f}\")\n",
    "    return cross_range, recalls, qps_list\n",
    "\n",
    "def plot_cross_pollination_results(cross_range, recalls, qps_list):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('N_CROSS (number of clusters each vector is inserted into)')\n",
    "    ax1.set_ylabel('Recall', color=color)\n",
    "    ax1.plot(cross_range, recalls, marker='o', color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('QPS', color=color)\n",
    "    ax2.plot(cross_range, qps_list, marker='x', color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    plt.title('Recall and QPS vs N_CROSS (cross-pollination)')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c69f0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_pollination_experiment(\n",
    "    dataset_path,\n",
    "    n_inner_clusters=400,\n",
    "    probe_strategy=\"nprobe\",\n",
    "    N_PROBE=2,\n",
    "    min_cross=1,\n",
    "    max_cross=6,\n",
    "    tshirt_size=\"small\"\n",
    "):\n",
    "    # Load data\n",
    "    xb, xq, gt = load_fashion_mnist(dataset_path)\n",
    "    d = xb.shape[1]\n",
    "    k = 10\n",
    "\n",
    "    # Build KMeans\n",
    "    inner_kmeans = build_kmeans(xb, d, n_inner_clusters)\n",
    "    _, xq_inner_assignments = inner_kmeans.index.search(xq, 1)\n",
    "    inner_centroids = inner_kmeans.centroids\n",
    "\n",
    "    outer_kmeans = build_kmeans(inner_centroids, d, 10)\n",
    "    _, inner_to_outer = outer_kmeans.index.search(inner_centroids, 1)\n",
    "    _, xq_outer_assignments = outer_kmeans.index.search(xq, 3)\n",
    "\n",
    "    # Evaluate cross-pollination\n",
    "    cross_range, recalls, qps_list = evaluate_cross_pollination(\n",
    "        xb, xq, gt, inner_kmeans, inner_to_outer, xq_outer_assignments, k, d,\n",
    "        N_PROBE=N_PROBE, min_cross=min_cross, max_cross=max_cross, probe_strategy=probe_strategy,\n",
    "        tshirt_size=tshirt_size, n_outer_total=10\n",
    "    )\n",
    "\n",
    "    # Plot results\n",
    "    plot_cross_pollination_results(cross_range, recalls, qps_list)\n",
    "    return cross_range, recalls, qps_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4eee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running experiment for fashion-mnist ===\n",
      "Sampling a subset of 25600 / 50000 for training\n",
      "Clustering 25600 points in 784D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.02 s\n",
      "  Iteration 19 (0.74 s, search 0.64 s): objective=3.38648e+10 imbalance=1.173 nsplit=0       \n",
      "Clustering 100 points in 784D to 10 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 19 (0.01 s, search 0.00 s): objective=1.01799e+08 imbalance=1.434 nsplit=0       \n",
      "Evaluating N_CROSS = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 100 points to 10 centroids: please provide at least 390 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of inner clusters for each outer cluster:\n",
      "Outer cluster 0: 22 inner clusters\n",
      "Outer cluster 1: 5 inner clusters\n",
      "Outer cluster 2: 6 inner clusters\n",
      "Outer cluster 3: 10 inner clusters\n",
      "Outer cluster 4: 23 inner clusters\n",
      "Outer cluster 5: 4 inner clusters\n",
      "Outer cluster 6: 5 inner clusters\n",
      "Outer cluster 7: 11 inner clusters\n",
      "Outer cluster 8: 7 inner clusters\n",
      "Outer cluster 9: 7 inner clusters\n",
      "N_CROSS=4: recall=0.1746, qps=15.37\n",
      "Evaluating N_CROSS = 5\n",
      "\n",
      "Number of inner clusters for each outer cluster:\n",
      "Outer cluster 0: 22 inner clusters\n",
      "Outer cluster 1: 5 inner clusters\n",
      "Outer cluster 2: 6 inner clusters\n",
      "Outer cluster 3: 10 inner clusters\n",
      "Outer cluster 4: 23 inner clusters\n",
      "Outer cluster 5: 4 inner clusters\n",
      "Outer cluster 6: 5 inner clusters\n",
      "Outer cluster 7: 11 inner clusters\n",
      "Outer cluster 8: 7 inner clusters\n",
      "Outer cluster 9: 7 inner clusters\n"
     ]
    }
   ],
   "source": [
    "# Specify which dataset to use: \"fashion-mnist\", \"gist\", or \"sift\"\n",
    "selected_dataset = \"fashion-mnist\"\n",
    "\n",
    "DATA_URLS = {\n",
    "    \"fashion-mnist\": \"http://ann-benchmarks.com/fashion-mnist-784-euclidean.hdf5\",\n",
    "    \"gist\": \"http://ann-benchmarks.com/gist-960-euclidean.hdf5\",\n",
    "    \"sift\": \"http://ann-benchmarks.com/sift-128-euclidean.hdf5\"\n",
    "}\n",
    "CACHES = {\n",
    "    name: os.path.join(tempfile.gettempdir(), url.split('/')[-1])\n",
    "    for name, url in DATA_URLS.items()\n",
    "}\n",
    "\n",
    "# Download the selected dataset if not present\n",
    "cache_path = CACHES[selected_dataset]\n",
    "download_fashion_mnist(cache_path, DATA_URLS[selected_dataset])\n",
    "\n",
    "# Run cross-pollination experiment for the selected dataset\n",
    "print(f\"\\n=== Running experiment for {selected_dataset} ===\")\n",
    "run_cross_pollination_experiment(\n",
    "    dataset_path=cache_path,\n",
    "    n_inner_clusters=100,\n",
    "    probe_strategy=\"nprobe\",\n",
    "    N_PROBE=5,\n",
    "    min_cross=4,\n",
    "    max_cross=5,\n",
    "    tshirt_size=\"small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c853d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for PQ\n",
    "pq_m = 8      # Number of PQ sub-vectors (should divide d)\n",
    "pq_nbits = 8  # Bits per sub-vector\n",
    "\n",
    "# --- Build PQ codebooks and encode inner clusters ---\n",
    "def build_pq_for_inner_clusters(xb, xb_inner_assignments, inner_centroids):\n",
    "    pq_codebooks = {}\n",
    "    pq_codes = {}\n",
    "    for inner_id in range(inner_centroids.shape[0]):\n",
    "        # Get all vectors assigned to this inner cluster\n",
    "        idxs = np.where(xb_inner_assignments[:,0] == inner_id)[0]\n",
    "        if len(idxs) == 0:\n",
    "            continue\n",
    "        # Compute residuals\n",
    "        residuals = xb[idxs] - inner_centroids[inner_id]\n",
    "        # Train PQ on residuals\n",
    "        pq = faiss.ProductQuantizer(residuals.shape[1], pq_m, pq_nbits)\n",
    "        pq.train(residuals)\n",
    "        # Encode residuals\n",
    "        codes = np.empty((len(residuals), pq.code_size), dtype='uint8')\n",
    "        pq.compute_codes(residuals, codes)\n",
    "        pq_codebooks[inner_id] = pq\n",
    "        pq_codes[inner_id] = (idxs, codes)\n",
    "    return pq_codebooks, pq_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9c82019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Search using PQ codes in inner clusters ---\n",
    "def search_query_pq_inner(x, inner_kmeans, inner_centroids, pq_codebooks, pq_codes, xb, k):\n",
    "    # Find closest inner cluster\n",
    "    _, inner_assign = inner_kmeans.index.search(x.reshape(1, -1), 1)\n",
    "    inner_id = int(inner_assign[0,0])\n",
    "    if inner_id not in pq_codebooks:\n",
    "        # fallback to brute-force\n",
    "        dists = np.linalg.norm(xb - x.reshape(1, -1), axis=1)\n",
    "        idx = np.argsort(dists)[:k]\n",
    "        return idx, dists[idx]\n",
    "    # Compute residual for query\n",
    "    residual = x - inner_centroids[inner_id]\n",
    "    pq = pq_codebooks[inner_id]\n",
    "    idxs, codes = pq_codes[inner_id]\n",
    "    # Compute distances from query residual to all PQ codes in this cluster\n",
    "    dis = pq.compute_distance_table(residual.reshape(1, -1)).reshape(-1, pq.ksub * pq.M)\n",
    "    # Use FAISS's search with PQ codes\n",
    "    # For each code, sum the lookup table entries\n",
    "    lookup = pq.compute_distance_table(residual.reshape(1, -1)).reshape(pq.M, pq.ksub)\n",
    "    dists = np.zeros(len(codes), dtype='float32')\n",
    "    for i, code in enumerate(codes):\n",
    "        dists[i] = sum(lookup[m, code[m]] for m in range(pq.M))\n",
    "    topk = np.argsort(dists)[:k]\n",
    "    return idxs[topk], dists[topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4b47475",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in void faiss::Clustering::train_encoded(idx_t, const uint8_t *, const Index *, Index &, const float *) at /Users/runner/miniconda3/conda-bld/faiss-pkg_1745590552381/work/faiss/Clustering.cpp:279: Error: 'nx >= k' failed: Number of training points (82) should be at least as large as number of clusters (256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5x/9b1bmtw92895rfkby7llgg_w0000gn/T/ipykernel_9168/1468940690.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Assign each vector to its closest inner cluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb_inner_assignments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner_kmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Build PQ codebooks and codes for each inner cluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpq_codebooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpq_codes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_pq_for_inner_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb_inner_assignments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_centroids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Search all queries using PQ in inner clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/5x/9b1bmtw92895rfkby7llgg_w0000gn/T/ipykernel_9168/2166078658.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(xb, xb_inner_assignments, inner_centroids)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Compute residuals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mresiduals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minner_centroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minner_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Train PQ on residuals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mpq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProductQuantizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresiduals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpq_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpq_nbits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresiduals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Encode residuals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresiduals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mpq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_codes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresiduals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/faiss/class_wrappers.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \"\"\"\n\u001b[1;32m    147\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/faiss/swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, n, x)\u001b[0m\n\u001b[1;32m   3182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3183\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProductQuantizer_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Error in void faiss::Clustering::train_encoded(idx_t, const uint8_t *, const Index *, Index &, const float *) at /Users/runner/miniconda3/conda-bld/faiss-pkg_1745590552381/work/faiss/Clustering.cpp:279: Error: 'nx >= k' failed: Number of training points (82) should be at least as large as number of clusters (256)"
     ]
    }
   ],
   "source": [
    "# --- Product Quantization Example ---\n",
    "# Assign each vector to its closest inner cluster\n",
    "_, xb_inner_assignments = inner_kmeans.index.search(xb, 1)\n",
    "\n",
    "# Build PQ codebooks and codes for each inner cluster\n",
    "pq_codebooks, pq_codes = build_pq_for_inner_clusters(xb, xb_inner_assignments, inner_centroids)\n",
    "\n",
    "# Search all queries using PQ in inner clusters\n",
    "I = []\n",
    "D = []\n",
    "for x in xq:\n",
    "    idxs, dists = search_query_pq_inner(x, inner_kmeans, inner_centroids, pq_codebooks, pq_codes, xb, k)\n",
    "    I.append(idxs)\n",
    "    D.append(dists)\n",
    "I = np.array(I)\n",
    "D = np.array(D)\n",
    "recall = (I == gt[:, :k]).sum() / (gt.shape[0] * k)\n",
    "print(f\"Recall@{k} using PQ in inner clusters: {recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
