{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "713ac146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "import numpy as np\n",
    "import faiss\n",
    "import h5py\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5f9624",
   "metadata": {},
   "source": [
    "# Product Quantization with Multi-Level Clustering\n",
    "\n",
    "This notebook demonstrates how to use Product Quantization (PQ) within a multi-level clustering approach for efficient vector similarity search. PQ compresses vectors by quantizing sub-vectors independently, reducing memory usage while maintaining search accuracy.\n",
    "\n",
    "## Key Concepts:\n",
    "- **Product Quantization**: Divides vectors into sub-vectors and quantizes each independently\n",
    "- **Residual Encoding**: Stores PQ codes of residuals after subtracting cluster centroids\n",
    "- **Multi-level Search**: Uses hierarchical clustering to organize data before applying PQ\n",
    "\n",
    "## Implementation Overview:\n",
    "1. Load and prepare dataset\n",
    "2. Build hierarchical K-means clustering\n",
    "3. Apply PQ to residuals within each cluster\n",
    "4. Implement PQ-based search\n",
    "5. Evaluate performance vs. baseline methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e35812c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading Utilities ---\n",
    "\n",
    "def download_dataset(cache_path, url, dataset_name):\n",
    "    \"\"\"Download dataset if not already cached.\"\"\"\n",
    "    if not os.path.exists(cache_path):\n",
    "        print(f\"Downloading {dataset_name} dataset (~300-500 MB)...\")\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            with open(cache_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded {dataset_name} successfully!\")\n",
    "        else:\n",
    "            raise Exception(f\"Failed to download dataset: HTTP{response.status_code}\")\n",
    "    else:\n",
    "        print(f\"{dataset_name} dataset already cached.\")\n",
    "\n",
    "def load_dataset(cache_path):\n",
    "    \"\"\"Load dataset from HDF5 file.\"\"\"\n",
    "    with h5py.File(cache_path, \"r\") as f:\n",
    "        xb = f[\"train\"][:].astype(np.float32)  # Base vectors\n",
    "        xq = f[\"test\"][:].astype(np.float32)   # Query vectors\n",
    "        gt = f[\"neighbors\"][:]                 # Ground truth\n",
    "    return xb, xq, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcc25c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- K-means Building Utilities ---\n",
    "\n",
    "def build_kmeans(data, d, n_clusters, niter=20, spherical=False):\n",
    "    \"\"\"Build K-means model.\"\"\"\n",
    "    # Sample data for training if dataset is large\n",
    "    train_sample = data[np.random.choice(len(data), size=min(50000, len(data)), replace=False)]\n",
    "    kmeans = faiss.Kmeans(d, n_clusters, niter=niter, verbose=True, spherical=spherical)\n",
    "    kmeans.train(train_sample)\n",
    "    return kmeans\n",
    "\n",
    "def assign_vectors_to_clusters(vectors, kmeans, n_assign=1):\n",
    "    \"\"\"Assign vectors to their closest clusters.\"\"\"\n",
    "    _, assignments = kmeans.index.search(vectors, n_assign)\n",
    "    return assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f1a4b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for PQ\n",
    "pq_m = 8      # Number of PQ sub-vectors (should divide d)\n",
    "pq_nbits = 8  # Bits per sub-vector\n",
    "\n",
    "# --- Build PQ codebooks and encode inner clusters ---\n",
    "def build_pq_for_inner_clusters(xb, xb_inner_assignments, inner_centroids):\n",
    "    pq_codebooks = {}\n",
    "    pq_codes = {}\n",
    "    for inner_id in range(inner_centroids.shape[0]):\n",
    "        # Get all vectors assigned to this inner cluster\n",
    "        idxs = np.where(xb_inner_assignments[:,0] == inner_id)[0]\n",
    "        if len(idxs) == 0:\n",
    "            continue\n",
    "        # Compute residuals\n",
    "        residuals = xb[idxs] - inner_centroids[inner_id]\n",
    "        \n",
    "        # Skip if too few residuals for PQ training\n",
    "        if len(residuals) < pq_m * (2 ** pq_nbits):\n",
    "            continue\n",
    "            \n",
    "        # Train PQ on residuals\n",
    "        pq = faiss.ProductQuantizer(residuals.shape[1], pq_m, pq_nbits)\n",
    "        pq.train(residuals)\n",
    "        \n",
    "        # Encode residuals using the correct API\n",
    "        codes = pq.compute_codes(residuals)\n",
    "        pq_codebooks[inner_id] = pq\n",
    "        pq_codes[inner_id] = (idxs, codes)\n",
    "    return pq_codebooks, pq_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b091d2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fashion-mnist dataset already cached.\n",
      "Dataset: fashion-mnist\n",
      "Base vectors: (60000, 784)\n",
      "Query vectors: (10000, 784)\n",
      "Ground truth: (10000, 100)\n",
      "Using PQ with m=8 sub-vectors (dimension=784)\n"
     ]
    }
   ],
   "source": [
    "# --- Dataset Configuration ---\n",
    "\n",
    "# Choose dataset: \"fashion-mnist\", \"gist\", or \"sift\"\n",
    "selected_dataset = \"fashion-mnist\"\n",
    "\n",
    "DATA_URLS = {\n",
    "    \"fashion-mnist\": \"http://ann-benchmarks.com/fashion-mnist-784-euclidean.hdf5\",\n",
    "    \"gist\": \"http://ann-benchmarks.com/gist-960-euclidean.hdf5\",\n",
    "    \"sift\": \"http://ann-benchmarks.com/sift-128-euclidean.hdf5\"\n",
    "}\n",
    "\n",
    "CACHES = {\n",
    "    name: os.path.join(tempfile.gettempdir(), url.split('/')[-1])\n",
    "    for name, url in DATA_URLS.items()\n",
    "}\n",
    "\n",
    "# Download and load the selected dataset\n",
    "cache_path = CACHES[selected_dataset]\n",
    "download_dataset(cache_path, DATA_URLS[selected_dataset], selected_dataset)\n",
    "xb, xq, gt = load_dataset(cache_path)\n",
    "\n",
    "print(f\"Dataset: {selected_dataset}\")\n",
    "print(f\"Base vectors: {xb.shape}\")\n",
    "print(f\"Query vectors: {xq.shape}\")\n",
    "print(f\"Ground truth: {gt.shape}\")\n",
    "\n",
    "d = xb.shape[1]\n",
    "k = 10  # Number of nearest neighbors to find\n",
    "\n",
    "# Adjust PQ parameters based on dimensionality\n",
    "if d % 8 == 0:\n",
    "    pq_m = 8\n",
    "elif d % 4 == 0:\n",
    "    pq_m = 4\n",
    "else:\n",
    "    pq_m = 2\n",
    "    \n",
    "print(f\"Using PQ with m={pq_m} sub-vectors (dimension={d})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53869ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building Multi-Level K-means Clustering ===\n",
      "Building 100 inner clusters...\n",
      "Sampling a subset of 25600 / 50000 for training\n",
      "Clustering 25600 points in 784D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.02 s\n",
      "  Iteration 19 (0.56 s, search 0.46 s): objective=3.37934e+10 imbalance=1.150 nsplit=0       \n",
      "Inner clustering completed. Centroids shape: (100, 784)\n",
      "\n",
      "Building 10 outer clusters from inner centroids...\n",
      "Clustering 100 points in 784D to 10 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 19 (0.56 s, search 0.46 s): objective=3.37934e+10 imbalance=1.150 nsplit=0       \n",
      "Inner clustering completed. Centroids shape: (100, 784)\n",
      "\n",
      "Building 10 outer clusters from inner centroids...\n",
      "Clustering 100 points in 784D to 10 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 0 (0.01 s, search 0.01 s): objective=1.77772e+08 imbalance=1.402 nsplit=0       \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 100 points to 10 centroids: please provide at least 390 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 19 (0.24 s, search 0.18 s): objective=9.44027e+07 imbalance=1.240 nsplit=0       \n",
      "Outer clustering completed.\n",
      "Mapped 100 inner clusters to 10 outer clusters\n",
      "Assigned 10000 queries to outer clusters\n"
     ]
    }
   ],
   "source": [
    "# --- Build Multi-Level K-means Clustering ---\n",
    "\n",
    "print(\"\\n=== Building Multi-Level K-means Clustering ===\")\n",
    "\n",
    "# Level 1: Inner clusters (fine-grained)\n",
    "n_inner_clusters = 100\n",
    "print(f\"Building {n_inner_clusters} inner clusters...\")\n",
    "inner_kmeans = build_kmeans(xb, d, n_inner_clusters)\n",
    "inner_centroids = inner_kmeans.centroids\n",
    "print(f\"Inner clustering completed. Centroids shape: {inner_centroids.shape}\")\n",
    "\n",
    "# Level 2: Outer clusters (coarse-grained) - cluster the inner centroids\n",
    "n_outer_clusters = 10\n",
    "print(f\"\\nBuilding {n_outer_clusters} outer clusters from inner centroids...\")\n",
    "outer_kmeans = build_kmeans(inner_centroids, d, n_outer_clusters)\n",
    "print(f\"Outer clustering completed.\")\n",
    "\n",
    "# Create mapping from inner clusters to outer clusters\n",
    "_, inner_to_outer = outer_kmeans.index.search(inner_centroids, 1)\n",
    "print(f\"Mapped {n_inner_clusters} inner clusters to {n_outer_clusters} outer clusters\")\n",
    "\n",
    "# Assign query vectors to outer clusters for search\n",
    "_, xq_outer_assignments = outer_kmeans.index.search(xq, 3)  # Top 3 outer clusters per query\n",
    "print(f\"Assigned {len(xq)} queries to outer clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43188b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Search using PQ codes in inner clusters ---\n",
    "def search_query_pq_inner(x, inner_kmeans, inner_centroids, pq_codebooks, pq_codes, xb, k):\n",
    "    # Find closest inner cluster\n",
    "    _, inner_assign = inner_kmeans.index.search(x.reshape(1, -1), 1)\n",
    "    inner_id = int(inner_assign[0,0])\n",
    "    if inner_id not in pq_codebooks:\n",
    "        # fallback to brute-force\n",
    "        dists = np.linalg.norm(xb - x.reshape(1, -1), axis=1)\n",
    "        idx = np.argsort(dists)[:k]\n",
    "        return idx, dists[idx]\n",
    "    # Compute residual for query\n",
    "    residual = x - inner_centroids[inner_id]\n",
    "    pq = pq_codebooks[inner_id]\n",
    "    idxs, codes = pq_codes[inner_id]\n",
    "    # Compute distances from query residual to all PQ codes in this cluster\n",
    "    dis = pq.compute_distance_table(residual.reshape(1, -1)).reshape(-1, pq.ksub * pq.M)\n",
    "    # Use FAISS's search with PQ codes\n",
    "    # For each code, sum the lookup table entries\n",
    "    lookup = pq.compute_distance_table(residual.reshape(1, -1)).reshape(pq.M, pq.ksub)\n",
    "    dists = np.zeros(len(codes), dtype='float32')\n",
    "    for i, code in enumerate(codes):\n",
    "        dists[i] = sum(lookup[m, code[m]] for m in range(pq.M))\n",
    "    topk = np.argsort(dists)[:k]\n",
    "    return idxs[topk], dists[topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfb8ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Baseline Search Methods ---\n",
    "\n",
    "def search_brute_force(xq, xb, k):\n",
    "    \"\"\"Brute force exact search for comparison.\"\"\"\n",
    "    print(\"Running brute force search...\")\n",
    "    start_time = time.time()\n",
    "    I = []\n",
    "    D = []\n",
    "    for x in xq:\n",
    "        dists = np.linalg.norm(xb - x.reshape(1, -1), axis=1)\n",
    "        idx = np.argsort(dists)[:k]\n",
    "        I.append(idx)\n",
    "        D.append(dists[idx])\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return np.array(I), np.array(D), elapsed_time\n",
    "\n",
    "def search_kmeans_only(xq, inner_kmeans, inner_centroids, xb, k):\n",
    "    \"\"\"Search using K-means clustering without PQ.\"\"\"\n",
    "    print(\"Running K-means only search...\")\n",
    "    start_time = time.time()\n",
    "    I = []\n",
    "    D = []\n",
    "    \n",
    "    # Pre-assign all base vectors to clusters\n",
    "    _, xb_assignments = inner_kmeans.index.search(xb, 1)\n",
    "    cluster_to_points = defaultdict(list)\n",
    "    for idx, cluster_id in enumerate(xb_assignments[:, 0]):\n",
    "        cluster_to_points[cluster_id].append(idx)\n",
    "    \n",
    "    for x in xq:\n",
    "        # Find closest clusters for query\n",
    "        _, query_assignments = inner_kmeans.index.search(x.reshape(1, -1), 3)\n",
    "        \n",
    "        best_ids = []\n",
    "        best_dists = []\n",
    "        \n",
    "        for cluster_id in query_assignments[0]:\n",
    "            if cluster_id in cluster_to_points:\n",
    "                point_ids = cluster_to_points[cluster_id]\n",
    "                candidates = xb[point_ids]\n",
    "                dists = np.linalg.norm(candidates - x.reshape(1, -1), axis=1)\n",
    "                best_ids.extend(point_ids)\n",
    "                best_dists.extend(dists)\n",
    "        \n",
    "        if best_ids:\n",
    "            # Get top-k from all candidates\n",
    "            combined = list(zip(best_dists, best_ids))\n",
    "            combined.sort()\n",
    "            top_k = combined[:k]\n",
    "            I.append([idx for _, idx in top_k])\n",
    "            D.append([dist for dist, _ in top_k])\n",
    "        else:\n",
    "            # Fallback to brute force for this query\n",
    "            dists = np.linalg.norm(xb - x.reshape(1, -1), axis=1)\n",
    "            idx = np.argsort(dists)[:k]\n",
    "            I.append(idx)\n",
    "            D.append(dists[idx])\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    return np.array(I), np.array(D), elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fdc5b43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search_query_pq_inner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m D \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m xq:\n\u001b[0;32m---> 12\u001b[0m     idxs, dists \u001b[38;5;241m=\u001b[39m \u001b[43msearch_query_pq_inner\u001b[49m(x, inner_kmeans, inner_centroids, pq_codebooks, pq_codes, xb, k)\n\u001b[1;32m     13\u001b[0m     I\u001b[38;5;241m.\u001b[39mappend(idxs)\n\u001b[1;32m     14\u001b[0m     D\u001b[38;5;241m.\u001b[39mappend(dists)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'search_query_pq_inner' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Product Quantization Example ---\n",
    "# Assign each vector to its closest inner cluster\n",
    "_, xb_inner_assignments = inner_kmeans.index.search(xb, 1)\n",
    "\n",
    "# Build PQ codebooks and codes for each inner cluster\n",
    "pq_codebooks, pq_codes = build_pq_for_inner_clusters(xb, xb_inner_assignments, inner_centroids)\n",
    "\n",
    "# Search all queries using PQ in inner clusters\n",
    "I = []\n",
    "D = []\n",
    "for x in xq:\n",
    "    idxs, dists = search_query_pq_inner(x, inner_kmeans, inner_centroids, pq_codebooks, pq_codes, xb, k)\n",
    "    I.append(idxs)\n",
    "    D.append(dists)\n",
    "I = np.array(I)\n",
    "D = np.array(D)\n",
    "recall = (I == gt[:, :k]).sum() / (gt.shape[0] * k)\n",
    "print(f\"Recall@{k} using PQ in inner clusters: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748aab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comprehensive Evaluation ---\n",
    "\n",
    "print(\"\\n=== Performance Evaluation ===\")\n",
    "\n",
    "# Run all methods and collect results\n",
    "results = {}\n",
    "\n",
    "# 1. Brute Force (Ground Truth)\n",
    "print(\"\\n1. Brute Force Search:\")\n",
    "I_bf, D_bf, time_bf = search_brute_force(xq[:100], xb, k)  # Use subset for speed\n",
    "qps_bf = len(xq[:100]) / time_bf\n",
    "print(f\"Time: {time_bf:.4f}s, QPS: {qps_bf:.2f}\")\n",
    "results['Brute Force'] = {'qps': qps_bf, 'recall': 1.0, 'time': time_bf}\n",
    "\n",
    "# 2. K-means Only\n",
    "print(\"\\n2. K-means Only Search:\")\n",
    "I_km, D_km, time_km = search_kmeans_only(xq, inner_kmeans, inner_centroids, xb, k)\n",
    "qps_km = len(xq) / time_km\n",
    "recall_km = np.mean([\n",
    "    len(set(I_km[i]) & set(gt[i, :k])) / k\n",
    "    for i in range(len(xq))\n",
    "])\n",
    "print(f\"Time: {time_km:.4f}s, QPS: {qps_km:.2f}, Recall: {recall_km:.4f}\")\n",
    "results['K-means Only'] = {'qps': qps_km, 'recall': recall_km, 'time': time_km}\n",
    "\n",
    "# 3. Product Quantization\n",
    "print(\"\\n3. Product Quantization Search:\")\n",
    "start_time = time.time()\n",
    "I_pq = []\n",
    "D_pq = []\n",
    "for x in xq:\n",
    "    idxs, dists = search_query_pq_inner(x, inner_kmeans, inner_centroids, pq_codebooks, pq_codes, xb, k)\n",
    "    I_pq.append(idxs)\n",
    "    D_pq.append(dists)\n",
    "time_pq = time.time() - start_time\n",
    "I_pq = np.array(I_pq)\n",
    "D_pq = np.array(D_pq)\n",
    "qps_pq = len(xq) / time_pq\n",
    "recall_pq = np.mean([\n",
    "    len(set(I_pq[i]) & set(gt[i, :k])) / k\n",
    "    for i in range(len(xq))\n",
    "])\n",
    "print(f\"Time: {time_pq:.4f}s, QPS: {qps_pq:.2f}, Recall: {recall_pq:.4f}\")\n",
    "results['Product Quantization'] = {'qps': qps_pq, 'recall': recall_pq, 'time': time_pq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d603890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Memory Usage Analysis ---\n",
    "\n",
    "print(\"\\n=== Memory Usage Analysis ===\")\n",
    "\n",
    "# Calculate memory usage for different approaches\n",
    "original_size = xb.nbytes  # Original vectors in bytes\n",
    "\n",
    "# K-means approach: store cluster assignments (4 bytes per vector)\n",
    "kmeans_size = len(xb) * 4 + inner_centroids.nbytes\n",
    "\n",
    "# PQ approach: store PQ codes\n",
    "pq_size = inner_centroids.nbytes  # Centroids\n",
    "for inner_id, (idxs, codes) in pq_codes.items():\n",
    "    pq_size += codes.nbytes\n",
    "\n",
    "print(f\"Original vectors: {original_size / (1024**2):.2f} MB\")\n",
    "print(f\"K-means approach: {kmeans_size / (1024**2):.2f} MB (compression: {original_size/kmeans_size:.1f}x)\")\n",
    "print(f\"PQ approach: {pq_size / (1024**2):.2f} MB (compression: {original_size/pq_size:.1f}x)\")\n",
    "\n",
    "results['Original Size (MB)'] = original_size / (1024**2)\n",
    "results['K-means Size (MB)'] = kmeans_size / (1024**2)\n",
    "results['PQ Size (MB)'] = pq_size / (1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ac7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Results Visualization ---\n",
    "\n",
    "print(\"\\n=== Results Summary ===\")\n",
    "print(f\"{'Method':<20} {'QPS':<10} {'Recall':<10} {'Time (s)':<12}\")\n",
    "print(\"-\" * 55)\n",
    "for method, metrics in results.items():\n",
    "    if 'qps' in metrics:\n",
    "        print(f\"{method:<20} {metrics['qps']:<10.2f} {metrics['recall']:<10.4f} {metrics['time']:<12.4f}\")\n",
    "\n",
    "# Create comparison plots\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Product Quantization Performance Analysis', fontsize=16)\n",
    "\n",
    "# Plot 1: QPS Comparison\n",
    "methods = [m for m in results.keys() if 'qps' in results[m]]\n",
    "qps_values = [results[m]['qps'] for m in methods]\n",
    "ax1.bar(methods, qps_values, color=['blue', 'orange', 'green'])\n",
    "ax1.set_ylabel('Queries Per Second (QPS)')\n",
    "ax1.set_title('Search Speed Comparison')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Recall Comparison\n",
    "recall_values = [results[m]['recall'] for m in methods]\n",
    "ax2.bar(methods, recall_values, color=['blue', 'orange', 'green'])\n",
    "ax2.set_ylabel('Recall@10')\n",
    "ax2.set_title('Search Accuracy Comparison')\n",
    "ax2.set_ylim(0, 1.1)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Memory Usage\n",
    "memory_methods = ['Original', 'K-means', 'PQ']\n",
    "memory_values = [\n",
    "    results['Original Size (MB)'],\n",
    "    results['K-means Size (MB)'],\n",
    "    results['PQ Size (MB)']\n",
    "]\n",
    "ax3.bar(memory_methods, memory_values, color=['red', 'orange', 'green'])\n",
    "ax3.set_ylabel('Memory Usage (MB)')\n",
    "ax3.set_title('Memory Efficiency Comparison')\n",
    "\n",
    "# Plot 4: QPS vs Recall Trade-off\n",
    "qps_plot = [results[m]['qps'] for m in methods]\n",
    "recall_plot = [results[m]['recall'] for m in methods]\n",
    "colors = ['blue', 'orange', 'green']\n",
    "for i, method in enumerate(methods):\n",
    "    ax4.scatter(recall_plot[i], qps_plot[i], s=100, c=colors[i], label=method, alpha=0.7)\n",
    "ax4.set_xlabel('Recall@10')\n",
    "ax4.set_ylabel('Queries Per Second (QPS)')\n",
    "ax4.set_title('Speed vs Accuracy Trade-off')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d683bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameter Sensitivity Analysis ---\n",
    "\n",
    "print(\"\\n=== PQ Parameter Sensitivity Analysis ===\")\n",
    "\n",
    "# Test different PQ configurations\n",
    "pq_configs = [\n",
    "    {'m': 2, 'nbits': 8},\n",
    "    {'m': 4, 'nbits': 8},\n",
    "    {'m': 8, 'nbits': 8},\n",
    "    {'m': 4, 'nbits': 4},\n",
    "    {'m': 4, 'nbits': 16}\n",
    "]\n",
    "\n",
    "if d >= 8:  # Only run if dimension allows\n",
    "    sensitivity_results = []\n",
    "    \n",
    "    for config in pq_configs:\n",
    "        if d % config['m'] != 0:\n",
    "            continue  # Skip if dimension not divisible by m\n",
    "            \n",
    "        print(f\"\\nTesting PQ with m={config['m']}, nbits={config['nbits']}\")\n",
    "        \n",
    "        # Temporarily override global PQ parameters\n",
    "        temp_pq_m = config['m']\n",
    "        temp_pq_nbits = config['nbits']\n",
    "        \n",
    "        # Rebuild PQ with new parameters\n",
    "        temp_pq_codebooks = {}\n",
    "        temp_pq_codes = {}\n",
    "        \n",
    "        for inner_id in range(inner_centroids.shape[0]):\n",
    "            idxs = np.where(xb_inner_assignments[:,0] == inner_id)[0]\n",
    "            if len(idxs) == 0:\n",
    "                continue\n",
    "            residuals = xb[idxs] - inner_centroids[inner_id]\n",
    "            pq = faiss.ProductQuantizer(residuals.shape[1], temp_pq_m, temp_pq_nbits)\n",
    "            pq.train(residuals)\n",
    "            codes = np.empty((len(residuals), pq.code_size), dtype='uint8')\n",
    "            pq.compute_codes(residuals, codes)\n",
    "            temp_pq_codebooks[inner_id] = pq\n",
    "            temp_pq_codes[inner_id] = (idxs, codes)\n",
    "        \n",
    "        # Test search with new PQ\n",
    "        start_time = time.time()\n",
    "        I_temp = []\n",
    "        for x in xq[:500]:  # Use subset for faster evaluation\n",
    "            idxs, _ = search_query_pq_inner(x, inner_kmeans, inner_centroids, \n",
    "                                          temp_pq_codebooks, temp_pq_codes, xb, k)\n",
    "            I_temp.append(idxs)\n",
    "        test_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        I_temp = np.array(I_temp)\n",
    "        test_qps = len(xq[:500]) / test_time\n",
    "        test_recall = np.mean([\n",
    "            len(set(I_temp[i]) & set(gt[i, :k])) / k\n",
    "            for i in range(len(I_temp))\n",
    "        ])\n",
    "        \n",
    "        # Calculate memory usage\n",
    "        temp_memory = inner_centroids.nbytes\n",
    "        for inner_id, (idxs, codes) in temp_pq_codes.items():\n",
    "            temp_memory += codes.nbytes\n",
    "        temp_compression = original_size / temp_memory\n",
    "        \n",
    "        sensitivity_results.append({\n",
    "            'm': config['m'],\n",
    "            'nbits': config['nbits'],\n",
    "            'recall': test_recall,\n",
    "            'qps': test_qps,\n",
    "            'compression': temp_compression,\n",
    "            'memory_mb': temp_memory / (1024**2)\n",
    "        })\n",
    "        \n",
    "        print(f\"  Recall: {test_recall:.4f}, QPS: {test_qps:.2f}, Compression: {temp_compression:.1f}x\")\n",
    "    \n",
    "    # Plot sensitivity analysis\n",
    "    if sensitivity_results:\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('PQ Parameter Sensitivity Analysis', fontsize=16)\n",
    "        \n",
    "        config_labels = [f\"m={r['m']}, bits={r['nbits']}\" for r in sensitivity_results]\n",
    "        \n",
    "        # Recall comparison\n",
    "        recalls = [r['recall'] for r in sensitivity_results]\n",
    "        ax1.bar(range(len(config_labels)), recalls)\n",
    "        ax1.set_xticks(range(len(config_labels)))\n",
    "        ax1.set_xticklabels(config_labels, rotation=45)\n",
    "        ax1.set_ylabel('Recall@10')\n",
    "        ax1.set_title('Recall vs PQ Configuration')\n",
    "        \n",
    "        # QPS comparison\n",
    "        qps_vals = [r['qps'] for r in sensitivity_results]\n",
    "        ax2.bar(range(len(config_labels)), qps_vals, color='orange')\n",
    "        ax2.set_xticks(range(len(config_labels)))\n",
    "        ax2.set_xticklabels(config_labels, rotation=45)\n",
    "        ax2.set_ylabel('QPS')\n",
    "        ax2.set_title('Speed vs PQ Configuration')\n",
    "        \n",
    "        # Compression comparison\n",
    "        compressions = [r['compression'] for r in sensitivity_results]\n",
    "        ax3.bar(range(len(config_labels)), compressions, color='green')\n",
    "        ax3.set_xticks(range(len(config_labels)))\n",
    "        ax3.set_xticklabels(config_labels, rotation=45)\n",
    "        ax3.set_ylabel('Compression Ratio')\n",
    "        ax3.set_title('Memory Efficiency vs PQ Configuration')\n",
    "        \n",
    "        # Trade-off plot\n",
    "        for i, result in enumerate(sensitivity_results):\n",
    "            ax4.scatter(result['recall'], result['qps'], s=100, \n",
    "                       label=config_labels[i], alpha=0.7)\n",
    "        ax4.set_xlabel('Recall@10')\n",
    "        ax4.set_ylabel('QPS')\n",
    "        ax4.set_title('Speed vs Accuracy Trade-off')\n",
    "        ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\"Skipping sensitivity analysis - dimension {d} too small for multiple PQ configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea12682",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Performance Trade-offs**:\n",
    "   - **Brute Force**: Perfect recall but slow (baseline)\n",
    "   - **K-means Only**: Good balance of speed and accuracy\n",
    "   - **Product Quantization**: Excellent compression with competitive search speed\n",
    "\n",
    "2. **Memory Efficiency**:\n",
    "   - PQ achieves significant memory compression (typically 10-50x)\n",
    "   - Trade-off between compression ratio and accuracy\n",
    "   - Higher compression with more sub-vectors (larger m) or fewer bits\n",
    "\n",
    "3. **Parameter Selection**:\n",
    "   - `m` (number of sub-vectors): Should divide dimension evenly\n",
    "   - `nbits`: 8 bits often provides good balance\n",
    "   - More bits = better accuracy but larger memory usage\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **For Memory-Constrained Applications**:\n",
    "   - Use PQ with m=8, nbits=8 as starting point\n",
    "   - Adjust based on available memory and accuracy requirements\n",
    "\n",
    "2. **For Speed-Critical Applications**:\n",
    "   - K-means clustering alone may be sufficient\n",
    "   - PQ adds compression benefits with minimal speed penalty\n",
    "\n",
    "3. **For Large-Scale Deployment**:\n",
    "   - Combine multi-level clustering with PQ\n",
    "   - Use coarse clustering to reduce search space\n",
    "   - Apply PQ within clusters for memory efficiency\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Advanced PQ Variants**:\n",
    "   - Optimized Product Quantization (OPQ)\n",
    "   - Additive Quantization (AQ)\n",
    "   - Composite Quantization (CQ)\n",
    "\n",
    "2. **Integration with Modern Indexes**:\n",
    "   - Combine with HNSW for graph-based search\n",
    "   - Use with LSH for approximate search\n",
    "   - Apply to transformer embeddings\n",
    "\n",
    "3. **Hardware Optimization**:\n",
    "   - GPU acceleration for PQ distance computations\n",
    "   - SIMD optimizations for CPU implementations\n",
    "   - Memory-mapped file storage for large datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
